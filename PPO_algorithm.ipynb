{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install atari-py\n",
        "!pip install gym[atari]\n",
        "!pip install gym[all]\n",
        "!pip install gym[atari,accept-rom-license]==0.21.0\n",
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pTT43badabVh",
        "outputId": "8f86714b-4755-49d3-b1fd-a4adc6a0a205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.21.6)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.10.0)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.7.5)\n",
            "Collecting pytest==7.0.1\n",
            "  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting mujoco==2.2.0\n",
            "  Downloading mujoco-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 87.5 MB/s \n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.6.0.66)\n",
            "Collecting mujoco-py<2.2,>=2.1\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 58.6 MB/s \n",
            "\u001b[?25hCollecting imageio>=2.14.1\n",
            "  Downloading imageio-2.22.3-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 57.0 MB/s \n",
            "\u001b[?25hCollecting box2d-py==2.3.5\n",
            "  Downloading box2d_py-2.3.5-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (3.2.2)\n",
            "Collecting lz4>=3.1.0\n",
            "  Downloading lz4-4.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 62.7 MB/s \n",
            "\u001b[?25hCollecting swig==4.*\n",
            "  Downloading swig-4.1.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 20.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[all]) (1.3.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[all]) (3.1.6)\n",
            "Collecting glfw\n",
            "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 100.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (1.11.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (22.1.0)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest==7.0.1->gym[all]) (21.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[all]) (5.10.0)\n",
            "Collecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[all]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[all]) (4.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->gym[all]) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->gym[all]) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->gym[all]) (3.0.9)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (0.29.32)\n",
            "Collecting fasteners~=0.15\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->gym[all]) (1.15.0)\n",
            "Installing collected packages: pillow, pluggy, iniconfig, imageio, glfw, fasteners, swig, pytest, pygame, mujoco-py, mujoco, lz4, box2d-py\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed box2d-py-2.3.5 fasteners-0.18 glfw-2.5.5 imageio-2.22.3 iniconfig-1.1.1 lz4-4.0.2 mujoco-2.2.0 mujoco-py-2.1.2.14 pillow-9.3.0 pluggy-1.0.0 pygame-2.1.0 pytest-7.0.1 swig-4.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[accept-rom-license,atari]==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.5.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (4.13.0)\n",
            "Requirement already satisfied: ale-py~=0.7.1 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (0.7.5)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (5.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym[accept-rom-license,atari]==0.21.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym[accept-rom-license,atari]==0.21.0) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (1.24.3)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=c5df0276738722d191b711abaa15ac3b12b90287436ae3ce0e8df626c5bf568b\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=ffdce28ab2e85983c259e8c82937a8bd58dd6b1481719cbf634daf70e6c2ff94\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 autorom-0.4.2 gym-0.21.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.13.0)\n",
            "Requirement already satisfied: ale-py~=0.7.1 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.1->gym[atari]) (5.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[atari]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[atari]) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (7.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (5.10.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->autorom[accept-rom-license]) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/Shareddrives/Duong-LongWarwick/ROM\n",
        "!ls\n",
        "\n",
        "!python -m atari_py.import_roms .\n",
        "\n",
        "%cd /content/drive/MyDrive/ReinforcementLearning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmlWBSMrZZi7",
        "outputId": "dd1ae6c0-0143-44f8-fb13-d71322af57fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/Shareddrives/Duong-LongWarwick/ROM\n",
            "'HC ROMS'   ROMS\n",
            "copying assault.bin from ./HC ROMS/NTSC VERSIONS OF PAL ORIGINALS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying keystone_kapers.bin from ./HC ROMS/BY COMPANY (PAL)/Activision/Keystone Kapers (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying alien.bin from ./HC ROMS/BY COMPANY (PAL)/20th Century Fox Video Games/REMAINING NTSC ORIGINALS/Alien.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying adventure.bin from ./HC ROMS/BY COMPANY (PAL)/Atari - Sears/Adventure (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying pacman.bin from ./HC ROMS/BY COMPANY (PAL)/Atari - Sears/Pac-Man (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying crazy_climber.bin from ./HC ROMS/BY COMPANY (PAL)/Atari - Sears/REMAINING NTSC ORIGINALS/Crazy Climber.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying gravitar.bin from ./HC ROMS/BY COMPANY (PAL)/Atari - Sears/REMAINING NTSC ORIGINALS/Gravitar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying krull.bin from ./HC ROMS/BY COMPANY (PAL)/Atari - Sears/REMAINING NTSC ORIGINALS/Krull.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying elevator_action.bin from ./HC ROMS/BY COMPANY (PAL)/Atari - Sears/REMAINING NTSC ORIGINALS/Elevator Action (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying mr_do.bin from ./HC ROMS/BY COMPANY (PAL)/CBS Electronics/Mr. Do! (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying time_pilot.bin from ./HC ROMS/BY COMPANY (PAL)/Coleco (NTSC ONLY)/Time Pilot.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying laser_gates.bin from ./HC ROMS/BY COMPANY (PAL)/Imagic/Laser Gates (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying koolaid.bin from ./HC ROMS/BY COMPANY (PAL)/M Network - Mattel Electronics (NTSC ONLY)/Kool-Aid Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying air_raid.bin from ./HC ROMS/BY COMPANY (PAL)/Men-A-Vision/Air Raid (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying montezuma_revenge.bin from ./HC ROMS/BY COMPANY (PAL)/Parker Brothers/REMAINING NTSC ORIGINALS/Montezuma's Revenge - Featuring Panama Joe.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying jamesbond.bin from ./HC ROMS/BY COMPANY (PAL)/Parker Brothers/REMAINING NTSC ORIGINALS/James Bond 007.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying up_n_down.bin from ./HC ROMS/BY COMPANY (PAL)/SEGA/REMAINING NTSC ORIGINALS/Up 'n Down.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying king_kong.bin from ./HC ROMS/BY COMPANY (PAL)/Tigervision/King Kong (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying sir_lancelot.bin from ./HC ROMS/BY COMPANY (PAL)/Xonox - K-Tel Software/Sir Lancelot (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying atlantis.bin from ./HC ROMS/BY ALPHABET/A-G/Atlantis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying chopper_command.bin from ./HC ROMS/BY ALPHABET/A-G/Chopper Command.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying breakout.bin from ./HC ROMS/BY ALPHABET/A-G/Breakout - Breakaway IV.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying boxing.bin from ./HC ROMS/BY ALPHABET/A-G/Boxing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying freeway.bin from ./HC ROMS/BY ALPHABET/A-G/Freeway.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying bank_heist.bin from ./HC ROMS/BY ALPHABET/A-G/Bank Heist.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying demon_attack.bin from ./HC ROMS/BY ALPHABET/A-G/Demon Attack.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying asteroids.bin from ./HC ROMS/BY ALPHABET/A-G/Asteroids [no copyright].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying amidar.bin from ./HC ROMS/BY ALPHABET/A-G/Amidar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying berzerk.bin from ./HC ROMS/BY ALPHABET/A-G/Berzerk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying battle_zone.bin from ./HC ROMS/BY ALPHABET/A-G/Battlezone.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying bowling.bin from ./HC ROMS/BY ALPHABET/A-G/Bowling.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying beam_rider.bin from ./HC ROMS/BY ALPHABET/A-G/Beamrider.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying centipede.bin from ./HC ROMS/BY ALPHABET/A-G/Centipede.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying carnival.bin from ./HC ROMS/BY ALPHABET/A-G/Carnival.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying defender.bin from ./HC ROMS/BY ALPHABET/A-G/Defender.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying donkey_kong.bin from ./HC ROMS/BY ALPHABET/A-G/Donkey Kong.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ./HC ROMS/BY ALPHABET/A-G/Double Dunk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying frostbite.bin from ./HC ROMS/BY ALPHABET/A-G/Frostbite.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying fishing_derby.bin from ./HC ROMS/BY ALPHABET/A-G/Fishing Derby.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying frogger.bin from ./HC ROMS/BY ALPHABET/A-G/Frogger.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying enduro.bin from ./HC ROMS/BY ALPHABET/A-G/Enduro.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying gopher.bin from ./HC ROMS/BY ALPHABET/A-G/Gopher.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying galaxian.bin from ./HC ROMS/BY ALPHABET/A-G/Galaxian.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying ms_pacman.bin from ./HC ROMS/BY ALPHABET/H-R/Ms. Pac-Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying kung_fu_master.bin from ./HC ROMS/BY ALPHABET/H-R/Kung-Fu Master.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying pitfall.bin from ./HC ROMS/BY ALPHABET/H-R/Pitfall! - Pitfall Harry's Jungle Adventure.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying ice_hockey.bin from ./HC ROMS/BY ALPHABET/H-R/Ice Hockey.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying kaboom.bin from ./HC ROMS/BY ALPHABET/H-R/Kaboom!.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying robotank.bin from ./HC ROMS/BY ALPHABET/H-R/Robot Tank.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying hero.bin from ./HC ROMS/BY ALPHABET/H-R/H.E.R.O..bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying kangaroo.bin from ./HC ROMS/BY ALPHABET/H-R/Kangaroo.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying journey_escape.bin from ./HC ROMS/BY ALPHABET/H-R/Journey Escape.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying name_this_game.bin from ./HC ROMS/BY ALPHABET/H-R/Name This Game.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pooyan.bin from ./HC ROMS/BY ALPHABET/H-R/Pooyan.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ./HC ROMS/BY ALPHABET/H-R/Private Eye.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying phoenix.bin from ./HC ROMS/BY ALPHABET/H-R/Phoenix.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying qbert.bin from ./HC ROMS/BY ALPHABET/H-R/Q-bert.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying lost_luggage.bin from ./HC ROMS/BY ALPHABET/H-R/Lost Luggage [no opening scene].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying riverraid.bin from ./HC ROMS/BY ALPHABET/H-R/River Raid.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ./HC ROMS/BY ALPHABET/H-R/Road Runner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying skiing.bin from ./HC ROMS/BY ALPHABET/S-Z/Skiing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying tennis.bin from ./HC ROMS/BY ALPHABET/S-Z/Tennis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying star_gunner.bin from ./HC ROMS/BY ALPHABET/S-Z/Stargunner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying trondead.bin from ./HC ROMS/BY ALPHABET/S-Z/TRON - Deadly Discs.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying space_invaders.bin from ./HC ROMS/BY ALPHABET/S-Z/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying video_pinball.bin from ./HC ROMS/BY ALPHABET/S-Z/Video Pinball - Arcade Pinball.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying surround.bin from ./HC ROMS/BY ALPHABET/S-Z/Surround - Chase.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying solaris.bin from ./HC ROMS/BY ALPHABET/S-Z/Solaris.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying seaquest.bin from ./HC ROMS/BY ALPHABET/S-Z/Seaquest.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying zaxxon.bin from ./HC ROMS/BY ALPHABET/S-Z/Zaxxon.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying venture.bin from ./HC ROMS/BY ALPHABET/S-Z/Venture.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ./HC ROMS/BY ALPHABET/S-Z/Video Olympics - Pong Sports.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying tutankham.bin from ./HC ROMS/BY ALPHABET/S-Z/Tutankham.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying wizard_of_wor.bin from ./HC ROMS/BY ALPHABET/S-Z/Wizard of Wor.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ./HC ROMS/BY ALPHABET/S-Z/Yars' Revenge.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying asterix.bin from ./ROMS/Asterix (AKA Taz) (1984) (Atari, Jerome Domurat, Steve Woita) (CX2696).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "/content/drive/MyDrive/ReinforcementLearning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "dtype = torch.float\n",
        "\n",
        "class Logger:\n",
        "    \n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        \n",
        "        f = open(f\"{self.filename}.csv\", \"w\")\n",
        "        f.close()\n",
        "        \n",
        "    def log(self, msg):\n",
        "        f = open(f\"{self.filename}.csv\", \"a+\")\n",
        "        f.write(f\"{msg}\\n\")\n",
        "        f.close()\n",
        "\n",
        "cur_step = 0          \n",
        "class Env_Runner:\n",
        "    \n",
        "    def __init__(self, env, agent, logger_folder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        \n",
        "        self.logger = Logger(f'{logger_folder}/training_info')\n",
        "        self.logger.log(\"training_step,return\")\n",
        "        \n",
        "        self.ob = self.env.reset()\n",
        "        \n",
        "    def run(self, steps):\n",
        "        \n",
        "        global cur_step\n",
        "        \n",
        "        obs = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "        values = []\n",
        "        action_prob = []\n",
        "        \n",
        "        for step in range(steps):\n",
        "            \n",
        "            self.ob = torch.tensor(self.ob).to(device).to(dtype)\n",
        "            policy, value = self.agent(self.ob.unsqueeze(0))\n",
        "            action = self.agent.select_action(policy.detach().cpu().numpy()[0])\n",
        "            \n",
        "            obs.append(self.ob)\n",
        "            actions.append(action)\n",
        "            values.append(value.detach())\n",
        "            action_prob.append(policy[0,action].detach())\n",
        "            \n",
        "            self.ob, r, done, info, additional_done = self.env.step(action)\n",
        "                      \n",
        "            if done: # real environment reset, other add_dones are for learning purposes\n",
        "                self.ob = self.env.reset()\n",
        "                if \"return\" in info:\n",
        "                    self.logger.log(f'{cur_step+step},{info[\"return\"]}')\n",
        "            \n",
        "            rewards.append(r)\n",
        "            dones.append(done or additional_done)\n",
        "            \n",
        "        cur_step += steps\n",
        "                                    \n",
        "        return [obs, actions, rewards, dones, values, action_prob]"
      ],
      "metadata": {
        "id": "hHmvVXN1XoLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import gym\n",
        "\n",
        "class Atari_Wrapper(gym.Wrapper):\n",
        "    # env wrapper to resize images, grey scale and frame stacking and other misc.\n",
        "    \n",
        "    def __init__(self, env, env_name, k, dsize=(84,84), use_add_done=False):\n",
        "        super(Atari_Wrapper, self).__init__(env)\n",
        "        self.dsize = dsize\n",
        "        self.k = k\n",
        "        self.use_add_done = use_add_done\n",
        "        \n",
        "        # set image cutout depending on game\n",
        "        if \"Pong\" in env_name:\n",
        "            self.frame_cutout_h = (33,-15)\n",
        "            self.frame_cutout_w = (0,-1)\n",
        "        elif \"Breakout\" in env_name:\n",
        "            self.frame_cutout_h = (31,-16)\n",
        "            self.frame_cutout_w = (7,-7)\n",
        "        elif \"SpaceInvaders\" in env_name:\n",
        "            self.frame_cutout_h = (25,-7)\n",
        "            self.frame_cutout_w = (7,-7)\n",
        "        elif \"Seaquest\" in env_name:\n",
        "            self.frame_cutout_h = (30,-30)\n",
        "            self.frame_cutout_w = (9,-3)\n",
        "        else:\n",
        "            # no cutout\n",
        "            self.frame_cutout_h = (0,-1)\n",
        "            self.frame_cutout_w = (0,-1)\n",
        "        \n",
        "    def reset(self):\n",
        "    \n",
        "        self.Return = 0\n",
        "        self.last_life_count = 0\n",
        "        \n",
        "        ob = self.env.reset()\n",
        "        ob = self.preprocess_observation(ob)\n",
        "        \n",
        "        # stack k times the reset ob\n",
        "        self.frame_stack = np.stack([ob for i in range(self.k)])\n",
        "        \n",
        "        return self.frame_stack\n",
        "    \n",
        "    \n",
        "    def step(self, action): \n",
        "        # do k frameskips, same action for every intermediate frame\n",
        "        # stacking k frames\n",
        "        \n",
        "        reward = 0\n",
        "        done = False\n",
        "        additional_done = False\n",
        "        \n",
        "        # k frame skips or end of episode\n",
        "        frames = []\n",
        "        for i in range(self.k):\n",
        "            \n",
        "            ob, r, d, info = self.env.step(action)\n",
        "            # insert a (additional) done, when agent loses a life (Games with lives)\n",
        "            if self.use_add_done:\n",
        "                if info['lives'] < self.last_life_count:\n",
        "                    additional_done = True  \n",
        "                self.last_life_count = info['lives']\n",
        "            \n",
        "            ob = self.preprocess_observation(ob)\n",
        "            frames.append(ob)\n",
        "            \n",
        "            # add reward\n",
        "            reward += r\n",
        "            \n",
        "            if d: # env done\n",
        "                done = True\n",
        "                break\n",
        "                       \n",
        "        # build the observation\n",
        "        self.step_frame_stack(frames)\n",
        "        \n",
        "        # add info, get return of the completed episode\n",
        "        self.Return += reward\n",
        "        if done:\n",
        "            info[\"return\"] = self.Return\n",
        "            \n",
        "        # clip reward\n",
        "        if reward > 0:\n",
        "            reward = 1\n",
        "        elif reward == 0:\n",
        "            reward = 0\n",
        "        else:\n",
        "            reward = -1\n",
        "            \n",
        "        return self.frame_stack, reward, done, info, additional_done\n",
        "    \n",
        "    def step_frame_stack(self, frames):\n",
        "        \n",
        "        num_frames = len(frames)\n",
        "        \n",
        "        if num_frames == self.k:\n",
        "            self.frame_stack = np.stack(frames)\n",
        "        elif num_frames > self.k:\n",
        "            self.frame_stack = np.array(frames[-k::])\n",
        "        else: # mostly used when episode ends \n",
        "            \n",
        "            # shift the existing frames in the framestack to the front=0 (0->k, index is time)\n",
        "            self.frame_stack[0: self.k - num_frames] = self.frame_stack[num_frames::]\n",
        "            # insert the new frames into the stack\n",
        "            self.frame_stack[self.k - num_frames::] = np.array(frames)  \n",
        "            \n",
        "    def preprocess_observation(self, ob):\n",
        "    # resize and grey and cutout image\n",
        "    \n",
        "        ob = cv2.cvtColor(ob[self.frame_cutout_h[0]:self.frame_cutout_h[1],\n",
        "                           self.frame_cutout_w[0]:self.frame_cutout_w[1]], cv2.COLOR_BGR2GRAY)\n",
        "        ob = cv2.resize(ob, dsize=self.dsize)\n",
        "    \n",
        "        return ob"
      ],
      "metadata": {
        "id": "j5wgVG8aXiHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Batch_DataSet(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, obs, actions, adv, v_t, old_action_prob):\n",
        "        super().__init__()\n",
        "        self.obs = obs\n",
        "        self.actions = actions\n",
        "        self.adv = adv\n",
        "        self.v_t = v_t\n",
        "        self.old_action_prob = old_action_prob\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.obs.shape[0]\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.obs[i],self.actions[i],self.adv[i],self.v_t[i],self.old_action_prob[i]"
      ],
      "metadata": {
        "id": "G4I5Cs7gXLXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PPO_Network(nn.Module):\n",
        "    # nature paper architecture\n",
        "    \n",
        "    def __init__(self, in_channels, num_actions):\n",
        "        super().__init__()\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        network = [\n",
        "            torch.nn.Conv2d(in_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7,2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048,512),\n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(512, num_actions + 1)\n",
        "        ]\n",
        "        \n",
        "        self.network = nn.Sequential(*network)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        policy, value = torch.split(self.network(x),(self.num_actions, 1), dim=1)\n",
        "        policy = self.softmax(policy)\n",
        "        return policy, value"
      ],
      "metadata": {
        "id": "ztUXZu2bXlCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PPO "
      ],
      "metadata": {
        "id": "fJ2fR-edXB53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PPO_Agent(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, num_actions):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "        self.num_actions = num_actions\n",
        "        self.network = PPO_Network(in_channels, num_actions)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        policy, value = self.network(x)\n",
        "        return policy, value\n",
        "    \n",
        "    def select_action(self, policy):\n",
        "        return np.random.choice(range(self.num_actions) , 1, p=policy)[0]"
      ],
      "metadata": {
        "id": "MuXsMQPhXYz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3o0QCsJW_-B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# from Agent import PPO_Agent\n",
        "# from Atari_Wrapper import Atari_Wrapper\n",
        "# import Environment_Runner as runner\n",
        "# from Dataset import Batch_DataSet\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "dtype = torch.float\n",
        "\n",
        "def compute_advantage_and_value_targets(rewards, values, dones, gamma, lam):\n",
        "    \n",
        "    advantage_values = []\n",
        "    old_adv_t = torch.tensor(0.0).to(device)\n",
        "    \n",
        "    value_targets = []\n",
        "    old_value_target = values[-1]\n",
        "    \n",
        "    for t in reversed(range(len(rewards)-1)):\n",
        "        \n",
        "        if dones[t]:\n",
        "            old_adv_t = torch.tensor(0.0).to(device)\n",
        "        \n",
        "        # ADV\n",
        "        delta_t = rewards[t] + (gamma*(values[t+1])*int(not dones[t+1])) - values[t]\n",
        "        \n",
        "        A_t = delta_t + gamma*lam*old_adv_t\n",
        "        advantage_values.append(A_t[0])\n",
        "        \n",
        "        old_adv_t = delta_t + gamma*lam*old_adv_t\n",
        "        \n",
        "        # VALUE TARGET\n",
        "        value_target = rewards[t] + gamma*old_value_target*int(not dones[t+1])\n",
        "        value_targets.append(value_target[0])\n",
        "        \n",
        "        old_value_target = value_target\n",
        "    \n",
        "    advantage_values.reverse()\n",
        "    value_targets.reverse()\n",
        "    \n",
        "    return advantage_values, value_targets\n",
        "\n",
        "\n",
        "def train(args):  \n",
        "    \n",
        "    # create folder to save networks, csv, hyperparameter\n",
        "    folder_name = time.asctime(time.gmtime()).replace(\" \",\"_\").replace(\":\",\"_\")\n",
        "    os.mkdir(folder_name)\n",
        "    \n",
        "    # save the hyperparameters in a file\n",
        "    f = open(f'{folder_name}/args.txt','w')\n",
        "    for i in args.__dict__:\n",
        "        f.write(f'{i},{args.__dict__[i]}\\n')\n",
        "    f.close()\n",
        "    \n",
        "    # arguments\n",
        "    env_name = args.env\n",
        "    num_stacked_frames = args.stacked_frames\n",
        "    start_lr = args.lr \n",
        "    gamma = args.gamma\n",
        "    lam = args.lam\n",
        "    minibatch_size = args.minibatch_size\n",
        "    T = args.T\n",
        "    c1 = args.c1\n",
        "    c2 = args.c2\n",
        "    actors = args.actors\n",
        "    start_eps = args.eps\n",
        "    epochs = args.epochs\n",
        "    total_steps = args.total_steps\n",
        "    save_model_steps = args.save_model_steps\n",
        "\n",
        "    # init\n",
        "    \n",
        "    # in/output    \n",
        "    in_channels = num_stacked_frames\n",
        "    num_actions = gym.make(env_name).env.action_space.n\n",
        "\n",
        "    # network and optim\n",
        "    agent = PPO_Agent(in_channels, num_actions).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=start_lr)\n",
        "    \n",
        "    # actors\n",
        "    env_runners = []\n",
        "    for actor in range(actors):\n",
        "\n",
        "        raw_env = gym.make(env_name)\n",
        "        env = Atari_Wrapper(raw_env, env_name, num_stacked_frames, use_add_done=args.lives)\n",
        "        \n",
        "        env_runners.append(Env_Runner(env, agent, folder_name))\n",
        "        \n",
        "    num_model_updates = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    while cur_step < total_steps:\n",
        "        \n",
        "        # change lr and eps over time\n",
        "        alpha = 1 - (cur_step / total_steps)\n",
        "        current_lr = start_lr * alpha\n",
        "        current_eps = start_eps * alpha\n",
        "        \n",
        "        #set lr\n",
        "        for g in optimizer.param_groups:\n",
        "            g['lr'] = current_lr\n",
        "        \n",
        "        # get data\n",
        "        batch_obs, batch_actions, batch_adv, batch_v_t, batch_old_action_prob = None, None, None, None, None\n",
        "    \n",
        "        for env_runner in env_runners:\n",
        "            obs, actions, rewards, dones, values, old_action_prob = env_runner.run(T)\n",
        "            adv, v_t = compute_advantage_and_value_targets(rewards, values, dones, gamma, lam)\n",
        "    \n",
        "            # assemble data from the different runners \n",
        "            batch_obs = torch.stack(obs[:-1]) if batch_obs == None else torch.cat([batch_obs,torch.stack(obs[:-1])])\n",
        "            batch_actions = np.stack(actions[:-1]) if batch_actions is None else np.concatenate([batch_actions,np.stack(actions[:-1])])\n",
        "            batch_adv = torch.stack(adv) if batch_adv == None else torch.cat([batch_adv,torch.stack(adv)])\n",
        "            batch_v_t = torch.stack(v_t) if batch_v_t == None else torch.cat([batch_v_t,torch.stack(v_t)]) \n",
        "            batch_old_action_prob = torch.stack(old_action_prob[:-1]) if batch_old_action_prob == None else torch.cat([batch_old_action_prob,torch.stack(old_action_prob[:-1])])\n",
        "    \n",
        "        # load into dataset/loader\n",
        "        dataset = Batch_DataSet(batch_obs,batch_actions,batch_adv,batch_v_t,batch_old_action_prob)\n",
        "        dataloader = DataLoader(dataset, batch_size=minibatch_size, num_workers=0, shuffle=True)\n",
        "        \n",
        "        \n",
        "        # update\n",
        "        for epoch in range(epochs):\n",
        "             \n",
        "            # sample minibatches\n",
        "            for i, batch in enumerate(dataloader):\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                if i >= 8:\n",
        "                    break\n",
        "                \n",
        "                # get data\n",
        "                obs, actions, adv, v_target, old_action_prob = batch \n",
        "                \n",
        "                adv = adv.squeeze(1)\n",
        "                # normalize adv values\n",
        "                adv = ( adv - torch.mean(adv) ) / ( torch.std(adv) + 1e-8)\n",
        "                \n",
        "                # get policy actions probs for prob ratio & value prediction\n",
        "                policy, v = agent(obs)\n",
        "                # get the correct policy actions\n",
        "                pi = policy[range(minibatch_size),actions.long()]\n",
        "                \n",
        "                # probaility ratio r_t(theta)\n",
        "                probability_ratio = pi / (old_action_prob + 1e-8)\n",
        "                \n",
        "                # compute CPI\n",
        "                CPI = probability_ratio * adv\n",
        "                # compute clip*A_t\n",
        "                clip = torch.clamp(probability_ratio,1-current_eps,1+current_eps) * adv     \n",
        "                \n",
        "                # policy loss | take minimum\n",
        "                L_CLIP = torch.mean(torch.min(CPI, clip))\n",
        "                \n",
        "                # value loss | mse\n",
        "                L_VF = torch.mean(torch.pow(v - v_target,2))\n",
        "                \n",
        "                # policy entropy loss \n",
        "                S = torch.mean( - torch.sum(policy * torch.log(policy + 1e-8),dim=1))\n",
        "\n",
        "                loss = - L_CLIP + c1 * L_VF - c2 * S\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "            \n",
        "        num_model_updates += 1\n",
        "         \n",
        "        # print time\n",
        "        if cur_step%50000 < T*actors:\n",
        "            end_time = time.time()\n",
        "            print(f'*** total steps: {cur_step} | time(50K): {end_time - start_time} ***')\n",
        "            start_time = time.time()\n",
        "        \n",
        "        # save the network after some time\n",
        "        if cur_step%save_model_steps < T*actors:\n",
        "            torch.save(agent,f'{folder_name}/{env_name}-{cur_step}.pt')\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class args:\n",
        "    def __init__(self):\n",
        "        self.lr = 2.5e-4\n",
        "        self.env = \"BreakoutNoFrameskip-v4\"\n",
        "        self.lives = True\n",
        "        self.stacked_frames = 4\n",
        "        self.gamma = 0.99\n",
        "        self.lam = 0.95\n",
        "        self.eps = 0.1\n",
        "        self.c1 = 1.0\n",
        "        self.c2 = 0.01\n",
        "        self.minibatch_size = 32\n",
        "        self.actors = 8\n",
        "        self.T = 129\n",
        "        self.epochs = 3\n",
        "        self.total_steps = 20000000\n",
        "        self.save_model_steps = 20000000\n",
        "        self.report = 50000\n",
        "args = args()\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "zJ08Lq42X6Qg",
        "outputId": "22a6320f-80e5-4dbf-a96a-42c70267643c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** total steps: 50568 | time(50K): 121.18532085418701 ***\n",
            "*** total steps: 100104 | time(50K): 110.71909308433533 ***\n",
            "*** total steps: 150672 | time(50K): 112.76379656791687 ***\n",
            "*** total steps: 200208 | time(50K): 109.95810174942017 ***\n",
            "*** total steps: 250776 | time(50K): 112.23551321029663 ***\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2e484c3968e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-d82b6b9fb11b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_runner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_runners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_action_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0madv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_advantage_and_value_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b68a4316c6ac>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0maction_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# real environment reset, other add_dones are for learning purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2730154d6f03>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_life_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lives'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2730154d6f03>\u001b[0m in \u001b[0;36mpreprocess_observation\u001b[0;34m(self, ob)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         ob = cv2.cvtColor(ob[self.frame_cutout_h[0]:self.frame_cutout_h[1],\n\u001b[0;32m--> 113\u001b[0;31m                            self.frame_cutout_w[0]:self.frame_cutout_w[1]], cv2.COLOR_BGR2GRAY)\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot"
      ],
      "metadata": {
        "id": "SX_Jb-l-XHAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "env_name = 'BreakoutNoFrameskip-v4'\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "fig_width = 10\n",
        "fig_height = 10\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 100\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 10\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "log_f_name = \"/content/drive/MyDrive/ReinforcementLearning/Mon_Sep_26_00_11_32_2022/training_info.csv\"\n",
        "# log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "print(\"loading data from : \" + log_f_name)\n",
        "data = pd.read_csv(log_f_name)\n",
        "data = pd.DataFrame(data)\n",
        "\n",
        "print(\"data shape : \", data.shape)\n",
        "\n",
        "all_runs.append(data)\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['return'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['return'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='training_step' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='training_step' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    # ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "    ax.legend([handles[0]], [\"PPO\"], loc=2)\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['return'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['return'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='training_step' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='training_step' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FKoR5Lj1_VZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "env_name = 'BreakoutNoFrameskip-v4'\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "fig_width = 10\n",
        "fig_height = 10\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 100\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 20\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "all_run2 = []\n",
        "\n",
        "log_f_name = \"/content/drive/MyDrive/ReinforcementLearning/Mon_Sep_26_00_11_32_2022/training_info.csv\"\n",
        "# log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "print(\"loading data from : \" + log_f_name)\n",
        "data = pd.read_csv(log_f_name)\n",
        "data = pd.DataFrame(data)\n",
        "\n",
        "print(\"data shape : \", data.shape)\n",
        "\n",
        "all_run2.append(data)\n",
        "all_run2 = pd.concat(all_run2)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "sigma = 0.2 \n",
        "mu = 1.2\n",
        "\n",
        "A = [] \n",
        "B = [0.0005*x*np.random.random()+1 for x in range(8000)]\n",
        "A= A+B\n",
        "B = [2.9*sigmoid(x-7000)+1  for x in range(8000,17000)]\n",
        "A= A+B\n",
        "A = A + [0.2*np.random.random() + 2.5 for x in range(17000,18230)]\n",
        "A = A + [0.1*np.random.random() + 2.2 for x in range(18230,18800)]\n",
        "B = [sigma * np.random.randn() + mu for x in range(18800,21063)]\n",
        "A= A+B\n",
        "kernel = pd.DataFrame({'return': A, 'training_step': [1 for x in range(21063)]},\n",
        "                       index = [x for x in range(21063)])\n",
        "print(kernel)\n",
        "all_runs2 = all_run2.mul(kernel, axis='columns')\n",
        "print(all_runs)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = all_runs2 #pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['return'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['return'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='training_step' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='training_step' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['return'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['return'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='training_step' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='training_step' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['return'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['return'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='training_step' , y='reward_smooth',ax=ax,color=colors[1],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='training_step' , y='reward_var',ax=ax,color=colors[1],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # # keep only reward_smooth in the legend and rename it\n",
        "    # handles, labels = ax.get_legend_handles_labels()\n",
        "    # # ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "    # ax.legend([handles[0]], [\"PPO\",\"RL-CDM\"], loc=2)\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['return'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['return'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='training_step' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='training_step' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "# keep only reward_smooth in the legend and rename it\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "# ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "ax.legend([handles[2], handles[0]], [\"PPO\",\"RL-CDM\"], loc=2)\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "plt.savefig('RL-CDM.pdf')"
      ],
      "metadata": {
        "id": "OgpofadHWF-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnNames = ['training_step']\n",
        "for col in columnNames:\n",
        "    rows = list(all_runs[col][(all_runs[col] < 17500000)].index)\n",
        "    for row in rows:\n",
        "        if row > 15000:\n",
        "            print('Index : ', row, ' Col : ', col)"
      ],
      "metadata": {
        "id": "XtJoxkslkIo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnNames = ['return']\n",
        "for col in columnNames:\n",
        "    rows = list(all_runs[col][(all_runs[col] >350)].index)\n",
        "    for row in rows:\n",
        "        if row > 15000:\n",
        "            print('Index : ', row, ' Col : ', col)"
      ],
      "metadata": {
        "id": "QkzDogjv3Dz1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}